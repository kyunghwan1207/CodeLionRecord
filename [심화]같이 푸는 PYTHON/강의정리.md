# [심화]같이 푸는 PYTHON

# Ch 2. 실시간 검색어 확인하기

## Step 1. 실시간 검색어 웹 크롤링 프로젝트 미리보기

```css

```

## Step 2. 크롤러 알아보기

### 크롤러(Crawler)

1. 기는 것
2. 파충류

우리는 기어다닐 것이다 → 웹사이트를 기어다니며 데이터를 모을 것이다.(우리 대신 크롤러가 ㅎㅎ)

### Web Crawling

크롤러를 사용해 웹페이지의 데이터를 추출하는 것

## Step 3. 블록 조립 키트

```bash
pip install requests
```

```bash
import reuqest

print(requests)

```

### 함수(Function)와 모듈(Module)

**함수:** 
자주사용되는 코드들을 묶어놓고 사용할 수 있게 함 이로인해 반복적인 작업을 줄일 수 있게 도와줌

**모듈:**

자주 사용되는 코드(함수, 클래스, 변수 등)을 모아놓은 파일

## Step 4. 블록 조립 키트 사용법

## Step 5. 요청하고 응답받기1

`requests.get()` == GET 요청보내는 함수

### Client와 Server

Client는 요청하는 존재, Server는 요청에 대한 응답을 return하는 존재

## Step 6. 요청하고 응답받기2

![](https://velog.velcdn.com/images/kyunghwan1207/post/fc3e3164-d41d-43fc-8753-3d4fe7c271ce/image.png)

![](https://velog.velcdn.com/images/kyunghwan1207/post/42287596-fcbc-4fec-a5f8-4b21c87d1fe4/image.png)

## Step 7. 요청하고 응답받기3

```python
import requests

url = "http://www.daum.net"
response = requests.get(url) # <Response [200]> -> 요청과 응답이 정상적으로 이루어졌음

print(response.text) # <html> ~ </html> 코드 다 읽어옴

# print(response.url)

# print(response.content)

# print(response.encoding) # -> 응답받은곳의 HTML의 인코딩방식(UTF-8)

# print(response.headers)

# print(response.json)

# print(response.links)

# print(response.ok)

# print(response.status_code)
```

# Step 8. Beautiful Soup

Beautiful Soup은 우리가 가져온 데이터를 의미있게 바꿔준다.
어떤 통에다가 정보를 담아준다. 통에는 칸막이가 있어서 그 문자열을 하나하나빼서 통에 가지런히 정리해준다.

```python
import requests
from bs4 import BeautifulSoup # bs4 모듈에서 BeautifulSoup 기능을 가져온다

url = "http://www.daum.net/"
response = requests.get(url)
# print(response.text) # type: <class: 'str'>
soup = BeautifulSoup(response.text, "html.parser"))  # type: <class: 'bs4.BeatuifulSoup'>

file = open("daum.html", "w") # daum.html 파일을 만듦
file.write(response.txt) # 파일에 response.txt <str> 을 쓴다 -> html 코드가 기록됨
file.close()

print(soup.title) # <title>Daum</title>
print(soup.title.string) # Daum
print(soup.span) # <span> ~ </span> 맨위의 하나만 출력됨
print(soup.findAll('span')) # 모든 span 태그 출력됨

```

### BeautifulSoup(<데이터>, <파싱방법>)

<데이터>: HTML, XML 이 들어갈 수 있다.

<파싱방법>: 파이썬에서 기본제공해주는 “html.parser” 사용한다.

※ parsing 우리의 데이터, 문서를 의미 있는 데이터로 분해한다.

![](https://velog.velcdn.com/images/kyunghwan1207/post/123d7b40-8a68-43c1-85c1-e681a774fce9/image.png)

**공통점:** 실시간 검색어는 `a태그(<a>)`, `class = “link_favorsch @~”`를 가지고 있다.
(href는 값이 달라서 공통점x)

## 실시간 검색어만 추출하기!

```python
import requests
from bs4 import BeautifulSoup # bs4 모듈에서 BeautifulSoup 기능을 가져온다
from datetime import datetime

url = "http://www.daum.net/"

soup = BeautifulSoup(response.text, "html.parser"))

# 모든 a태그 출력하기
print(soup.findAll('a')) # a태그 안에 묶여있는 다른 태그들도 함께 출력됨

# a태그 중 link_favorsch라는 클래스 명을 가진 태그만 출력
results = soup.findAll('a', "link_favorsch") # type: <list>

print(datetime.today().strftime("%Y년 %m월 %d일의 실시간 검색어 순위입니다.\n")) # 오늘날짜 출력

rank = 1
for result in results:
	print(rank, '위:', result.get_text(), '\n')
	rank += 1
```

# Step 15. 파일로 출력하기
